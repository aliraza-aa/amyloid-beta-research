GERun: 
GERun: Note: Lines like this one prefixed with "GERun:" are for debugging
GERun:       purposes only and you do not need to report them to rc-support
GERun:       unless your job fails for other reasons.
GERun: 
GERun: 
GERun: Grid Engine parallel launcher abstraction layer version iv (public)
GERun: Dr Owain Kenway, RCAS, RITS, ISD, UCL, 7th of February, 2018
GERun: 
GERun: For licensing terms, see LICENSE.txt
GERun: 
GERun: Using environment: openmpi-sge
GERun: Running on 42 slots:
GERun:    42 MPI tasks
GERun:     1 threads per task
GERun: TMPDIR=/tmpdir/job/444640.undefined
GERun: 
GERun: Contents of machinefile:
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-018
GERun: node-c11b-005
GERun: node-c11b-005
GERun: 
GERun: GErun command being run:
GERun:  mpirun gmx_mpi mdrun -s invacuo_input.tpr -o invacuo.trr -x invacuo.xtc -c collapsed.gro -g invacuo.log -e invacuo.edr -v
                :-) GROMACS - gmx mdrun, 2022.5-plumed_2.9.0 (-:

Executable:   /home/zcbtar9/software/gromacs-2022.5-plumed-2.9.0-sp/bin/gmx_mpi
Data prefix:  /home/zcbtar9/software/gromacs-2022.5-plumed-2.9.0-sp
Working dir:  /lustre/home/zcbtar9/projects/amyloid-beta-research/ab40/system/setup
Command line:
  gmx_mpi mdrun -s invacuo_input.tpr -o invacuo.trr -x invacuo.xtc -c collapsed.gro -g invacuo.log -e invacuo.edr -v


Back Off! I just backed up invacuo.log to ./#invacuo.log.1#
Reading file invacuo_input.tpr, VERSION 2022.5-plumed_2.9.0 (single precision)
Changing nstlist from 25 to 100, rlist from 1.2 to 1.2

Using 42 MPI processes

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread per MPI process

starting mdrun 'Protein'
500000 steps,   1000.0 ps.

Step 0, time 0 (ps)  LINCS WARNING
relative constraint deviation after LINCS:
rms 0.764680, max 6.710029 (between atoms 321 and 322)
bonds that rotated more than 30 degrees:
 atom 1 atom 2  angle  previous, current, constraint length
    281    283   63.5    0.1111   0.1115      0.1111
    281    284   33.8    0.1111   0.1098      0.1111
    321    322   90.0    0.1080   0.8327      0.1080
Wrote pdb files with previous and current coordinates
step 0

Step 1, time 0.002 (ps)  LINCS WARNING
relative constraint deviation after LINCS:
rms 4.643009, max 40.551083 (between atoms 281 and 283)
bonds that rotated more than 30 degrees:
 atom 1 atom 2  angle  previous, current, constraint length
    277    278   90.0    0.1080   0.4998      0.1080
    281    282   90.0    0.1114   0.2106      0.1111
    281    283   90.0    0.1115   4.6163      0.1111
    281    284   90.0    0.1098   0.2506      0.1111
    321    322   90.0    0.8327   0.1108      0.1080
Wrote pdb files with previous and current coordinates

-------------------------------------------------------
Program:     gmx mdrun, version 2022.5-plumed_2.9.0
Source file: src/gromacs/ewald/pme_redistribute.cpp (line 305)
MPI rank:    20 (out of 42)

Fatal error:
1 particles communicated to PME rank 20 are more than 2/3 times the cut-off
out of the domain decomposition cell of their charge group in dimension x.
This usually means that your system is not well equilibrated.

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 20 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
